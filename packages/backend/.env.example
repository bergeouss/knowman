# KnowMan Backend Configuration
# Copy this file to .env and update values as needed

# ====================
# Server Configuration
# ====================
NODE_ENV=development
PORT=3001
HOST=localhost
VERSION=0.1.0

# ==============
# CORS Settings
# ==============
# Comma-separated list of allowed origins
CORS_ORIGINS=http://localhost:3000,http://localhost:5173

# ==============
# Database (SQLite)
# ==============
DATABASE_URL=sqlite:./data/knowman.db
DATABASE_LOGGING=false

# ==============
# Redis (Required for background processing)
# ==============
# Connection string format: redis://[:password@]host[:port][/db-number]
REDIS_URL=redis://localhost:6379
# Alternative: Use host/port separately if connection string doesn't work
# REDIS_HOST=localhost
# REDIS_PORT=6379
# REDIS_PASSWORD=  # Optional password
REDIS_DB=0
# Force IPv4 if having IPv6 connection issues
REDIS_FAMILY=4  # 4 for IPv4, 6 for IPv6, 0 for both (default)

# ==============
# Security
# ==============
# Change this in production!
JWT_SECRET=change-me-in-production
# API_KEY=  # Optional API key for external integrations

# ============================================
# AI Provider Selection
# ============================================
# Choose one of: openai, anthropic, gemini, llamacpp, mock
# Default: mock (no actual AI calls, uses basic algorithms)
AI_PROVIDER=mock

# ============================================
# OpenAI Configuration (when AI_PROVIDER=openai)
# ============================================
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=1000

# ============================================
# Anthropic Configuration (when AI_PROVIDER=anthropic)
# ============================================
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your-anthropic-api-key-here
ANTHROPIC_MODEL=claude-3-haiku-20240307
ANTHROPIC_TEMPERATURE=0.7
ANTHROPIC_MAX_TOKENS=1000

# ============================================
# Gemini Configuration (when AI_PROVIDER=gemini)
# ============================================
# Get your API key from: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your-gemini-api-key-here
GEMINI_MODEL=gemini-1.5-flash
GEMINI_TEMPERATURE=0.7
GEMINI_MAX_OUTPUT_TOKENS=1000

# ============================================
# Llama.cpp Configuration (when AI_PROVIDER=llamacpp)
# ============================================
# Requires running llama.cpp server locally
# Example: ./server -m models/llama-2-7b.gguf -c 2048 --port 8080
LLAMACPP_BASE_URL=http://localhost:8080
LLAMACPP_MODEL=llama2
LLAMACPP_TEMPERATURE=0.8
LLAMACPP_MAX_TOKENS=1000

# ============================================
# Embedding Provider Configuration
# ============================================
# Separate provider for embeddings (can be different from main AI provider)
# Choose one of: openai, anthropic, gemini, llamacpp, mock
# Default: same as AI_PROVIDER
EMBEDDING_PROVIDER=${AI_PROVIDER}

# OpenAI Embedding Model (when EMBEDDING_PROVIDER=openai)
OPENAI_EMBEDDING_MODEL=text-embedding-ada-002

# Llama.cpp Embedding Model (when EMBEDDING_PROVIDER=llamacpp)
# Note: Llama.cpp may need specific configuration for embeddings
LLAMACPP_EMBEDDING_MODEL=${LLAMACPP_MODEL}

# ============================================
# Common AI Settings
# ============================================
# Request timeout in milliseconds
AI_REQUEST_TIMEOUT=30000

# Number of retries for failed AI requests
AI_MAX_RETRIES=3

# Enable/disable AI request logging
AI_ENABLE_LOGGING=true

# ==============
# Processing Settings
# ==============
MAX_CONTENT_LENGTH=100000
PROCESSING_WORKERS=2
PROCESSING_TIMEOUT=30000

# ==============
# Logging
# ==============
LOG_LEVEL=info
LOG_PRETTY=true